\documentclass[a4paper,12pt,titlepage]{article} %page properties
\usepackage[headings]{fullpage} %for fullpage with headings
\usepackage{fancyhdr} %for headings
\usepackage{graphicx} %for label
\usepackage{tabularx} %for fullpage tables
\usepackage{longtable} %for tables on multiple pages
\usepackage{hyperref} %for links
\usepackage{mathtools} %for math symbols

% defining headings
\pagestyle{fancy}
\fancyhead{}
\fancyhead[LE,RO]{PAMI 2015-2016 \hfill EXAMS 2014-2015 SUMMARY}

\begin{document}
	\begin{titlepage}
		
		%defining university
		\begin{center}
			POLITECNICO DI MILANO --- COMO CAMPUS\\
			\vspace{10pt}
			\includegraphics[scale=0.1]{logo-polimi.png}\\
			\vspace{10pt}
			PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2015-2016\\
			prof. Matteo Matteucci	
			\line(2,0){500}
		\end{center}
		
		\vspace{60pt}
                xesh		
		%defining work type
		\begin{center}
			{\Huge \textbf{Exams 2014-2015}}\\
			\vspace{20pt}	
			{\Huge \textbf{Summary}}\\
		\end{center}
		
		\vspace{60pt}
		
		%defining project repository
		\begin{center}
			{\large Project Repository}
		\end{center}
		\begin{tabularx}{\textwidth}{|X|}
			\hline
			\href{https://github.com/attillax/PAMI-2015}{Click}\\
			\hline
		\end{tabularx}
		
		\vspace{20pt}
		
		%defining team members
		\begin{center}
			{\large Team Members}
		\end{center}
		\begin{tabularx}{\textwidth}{|X|X|X|}
			\hline
			ID & Surname & Name\\
			\hline
			10460625 & Golubeva & Svetlana\\
			\hline
			&  & \\
			\hline
			&  & \\
			\hline
			&  & \\
			\hline
		\end{tabularx}
		
		\vspace{\fill}
		\begin{center}
			\line(2,0){500}
		\end{center}
		
	\end{titlepage}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\tableofcontents

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\newpage
\listoftables

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%\newpage
\listoffigures

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\newpage
\section{Statistical learning (8 points)}
\subsection{ex 2015-02-09}
Answer the following questions:
\begin{enumerate}
\item Describe what are the bias, variance, and irreducible error of a model, how are they related with its complexity, how they are related to the expected prediction error, and what is the meaning of “bias-variance tradeoff”?
\item Draw a plot of (1) bias, (2) variance, (3) training error, (4) test error, and (5) irreducible error curves as a function of increasing amount of flexibility in a statistical learning method. Explain the reason of their shapes and highlight the relationships among them.
\end{enumerate}

\subsection{ex 2015-02-23}
In statistical learning theory, Test and Training set Mean Squared Errors are related by the so called Bias-Variance trade-off:
\begin{itemize}
\item[1.] Write and comment the formula representing the Bias-Variance trade off for the Expected Prediction Error in Regression
\end{itemize}

The previous formula does not hold for Classification, but a useful result exists for the Classification Error Rate as well

\begin{itemize}
\item[2.] Write and comment what statistical learning theory states about the minimum achievable average test error rate.
\end{itemize}

Provided the previous result for Classification, answer the following questions

\begin{itemize}
\item[3.] Describe in detail how the previous result for the optimal classifier is used to derive the Logistic Regression classifier; provide a detailed description of the underlining model for Logistic Regression and derive the shape of the decision boundary for Logistic Regression.
\item[4.] Describe in detail how the previous result for the optimal classifier is used to derive Linear Discriminant Analysis; provide a detailed description of the underlining model for Linear Discriminant Analysis and derive the shape of the decision boundary for Linear Discriminant Analysis.
\end{itemize}


\subsection{ex 2015-07-06}

\subsection{ex 2015-09-14}

\begin{center}
  \begin{tabular}{c|c}
    X & Class \\
    \hline
    \hline
    0 & A \\
    1 & A \\
    2 & A \\
    1 & A \\
    3 & A \\
    \hline
    1 & B \\
    2 & B \\
    3 & B \\
    4 & B \\
    \hline
    4 & C \\
    6 & C \\
    6 & C \\
    6 & C \\
  \end{tabular}
\end{center}

\subsection{ex 2015-09-30}

\section{Linear regression (8 points)}
\subsection{ex 2015-02-09}
Given the variables x = \{1, 2, 3, 4, 5, 6, 7, 8, 9\} and y = \{3.3, 3.6, 5.2, 5.6, 7.4, 8.3, 8.7, 9.7, 11.2\}
\begin{enumerate}
\item Manually compute the parameters $\hat{\beta}_{0} $ and $\hat{\beta}_{1} $ of a linear model $ \hat{y} = \hat{\beta}_{0} + \hat{\beta}_{1} x$ which fits the given data
\item What is the value of MSE calculated between the values of y and the ones returned by the $\hat{y}$ function?
\item How can we compute if the trend identified by $\hat{\beta}_{1}$ is significant or it is just due to spurious correlations?
\end{enumerate}
 
To ease your computation, you can follow the following steps:
\begin{itemize}
\item calculate the mean $\bar{x}$ of x
\item calculate the mean $\bar{y}$ of y
\item calculate $ x - \bar{x} $ (a vector where each value is $ x_{i} - \bar{x} $̄ )
\item calculate $ y - \bar{y} $ (as above)
\item calculate $ \hat{\beta}_{1} = \frac{\sum_{i=1}^{n} (x_{i}-\bar{x}) (y_{i}-\bar{y})}{ \sum_{i=1}^{n} (x_{i}-\bar{x})^{2} } $
\item calculate $ \hat{\beta}_{o} = \hat{y} - \hat{\beta}_{1} x $
\end{itemize}

\subsection{ex 2015-02-23}
Provide detailed answers to the following
\begin{enumerate}
\item Let assume you have a dataset with n=1000 observations and you try to fit different
  models on the data:
  \begin{itemize}
\item A linear regression model, i.e. $ Y = \beta_{0} + \beta_{1} X + \epsilon $
\item The polynomial regression model $ Y = \beta_{0} + \beta_{1} X + \beta_{2} X^{2} + \beta_{3} X^{3} + \beta_{4} X^{4} +  \epsilon $
\item A smoothing spline (i.e. an even more flexible model than the previous two)
  \end{itemize}
 For each of the three models, you calculate both training and test RSS. How would you expect the values of RSS to be (both in the training and in the test case), supposing that the true relationship between X and Y is (a) linear or (b) cubic?
\item What is the additive assumption in a linear regression model? Show how you would detect and quantify a possible interaction between the variables in a regression model and how you would model it from a statistical perspective. Finally explain, with an example, how your model would take this interaction into account (e.g. explain how, given a change in the input, the dependent variable – and consequently the output – change).
\end{enumerate}

\subsection{ex 2015-07-06}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

\subsection{ex 2015-09-14}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

\subsection{ex 2015-09-30}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

\section{Classification (8 points)}
\subsection{ex 2015-02-09}
Given the two sets of samples from classes RED = \{(1, 3), (2, 2), (3.5, 1), (5, 4), (1.5, 4), (4, 2)\} and GREEN = \{(2, 3), (3, 0.5), (4, 3), (3.5, 2), (1, 2), (2, 1)\}, and the three unclassified elements a = (4, 1), b = (2, 3.5), and c = (3, 4)
\begin{enumerate}
\item Use the KNN approach to classify the unknown items for k = 1, 3, 5. Apply the Euclidean distance as a metric (note that you can skip the actual distance calculations if you can tell the nearest neighbours at a glance).
\item Describe in details the Linear Discriminant Analysis and Logistic Regression techniques for classification and discuss how these techniques perform, in general, with respect to the KNN classifiers.
\end{enumerate}

\subsection{ex 2015-02-23}
Consider the following dataset with three classes and the Linear Discriminant Analysis
model (LDA hereafter)
\begin{center}
  \begin{tabular}{cc|c}
    X1 & X2  & Class \\
    \hline
    \hline
    1 & 1 & A \\
    2 & 2 & A \\
    2 & 3 & A \\
    3 & 1 & A \\
    4 & 1 & A \\
    \hline
    1 & 4 & B \\
    2 & 5 & B \\
    3 & 4 & B \\
    4 & 5 & B \\
    \hline
    4 & 3 & C \\
    6 & 1 & C \\
    6 & 2 & C \\
    6 & 3 & C \\
    \hline
  \end{tabular}
\end{center}

\begin{enumerate}
\item Compute the parameters of a LDA classifier
\item Compute the discriminant functions for the LDA classifier
\item Compute the equations of the boundaries between the three classes according to
the classifier
\item Draw the dataset and the boundaries between the classes according to the classifier
\item Compute the parameters in case of a Quadratic Discriminant Analysis model
\end{enumerate}

\subsection{ex 2015-07-06}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

\subsection{ex 2015-09-14}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

\subsection{ex 2015-09-30}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

\section{Clustering (8 points)}
\subsection{ex 2015-02-09}
Suppose you want to evaluate the results of some clustering algorithms using SSE and Accuracy.
\begin{enumerate}
\item Which of these measures is defined as “internal”, which is “external”, and what does this mean? 
\item After running your function, you obtain the result SSE=21.5. How can you evaluate whether this is a good or bad result? What would you compare this result with?
\item One of the clustering algorithms allows you to choose the number of clusters in advance. You calculate SSE after different executions of this algorithm, using K=2, 3, ..., 10. SSE for K=10 provides the lowest value: what can you deduce from this?
\item Now suppose you have ground truth for your dataset. You run two different clustering algorithms on the same data and obtain the following results:

\begin{center}
	\begin{tabular}{c|cc}
                & SSE & Accuracy\\
                \hline
                Algorithm1 & 115.3 & 87\% \\
                Algorithm2 & 1285 & 95\% \\
	\end{tabular}
\end{center}

What is the meaning of these results? Which algorithm is better?
\end{enumerate}

\subsection{ex 2015-02-23}
Given the two datasets shown in figure (where blue diamonds are data points and red circles different centroids starting points), calculate and show the different steps of the K-Means algorithm for both the examples in the following way:
\begin{itemize}
\item At each step, specify the initial positions of the centroids
\item Without actually calculating it (unless it is needed to verify distances you cannot tell apart at a glance), for each step specify which centroid the various dataset points belong to
\item After you have assigned data points to the different centroids, calculate their new positions and proceed to next step
\item 
\end{itemize}

Tell how many iterations the algorithm needs to converge, compare its behavior in the two cases and write a comment about it: is the final situation the one you might expect/desire? If not, explain why.\\

\{PIC1 \} \{PIC2 \}

\subsection{ex 2015-07-06}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

\subsection{ex 2015-09-14}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

\subsection{ex 2015-09-30}
\begin{enumerate}
\item
\item
\item
\item 
\end{enumerate}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\end{document}
